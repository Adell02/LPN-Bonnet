training:
  seed: 0
  resume_from_checkpoint: null  # null to start from scratch
  inference_mode: mean  # mean, all, random_search, gradient_ascent
  inference_kwargs:
  batch_size: 128  # divisible by gradient_accumulation_steps * num_devices
  gradient_accumulation_steps: 1
  total_num_steps: 7000
  log_every_n_steps: 5
  eval_every_n_logs: 20  # null to disable eval
  save_checkpoint_every_n_logs: 10  # null to disable checkpointing
  learning_rate: 4e-4
  prior_kl_coeff: 1e-3
  
  pairwise_kl_coeff: null
  mixed_precision: False  # if True, uses bfloat16 activations
  online_data_augmentation: False
  # Structured uniform loading (balanced patterns 1/2/3, 4 pairs per task, consistent color/pattern per task)
  struct_uniform: true
  struct_uniform_length: 1024  # total training samples; will be split equally among patterns
  struct_num_pairs: 4
  # Legacy on-the-fly generator (ignored when struct_uniform=true)
  task_generator:
    class: STRUCT_PATTERN
    num_workers: 4
    num_pairs: 4
    pattern: 0 # Mix all three patterns by sampling per pair (pattern: 0)
    pattern_per_task: true
    num_rows: 5
    num_cols: 5
  train_datasets:
  
  use_hf: False

eval:
  # Use balanced, uniform evaluation unless you override test_datasets below
  struct_uniform: true
  struct_uniform_length: 96   # 32 per pattern
  batch_size: 96
  eval_datasets: []  # Keep empty
  test_datasets: []  # Empty when using struct_uniform

  json_datasets:

encoder_transformer:
  _target_:  models.utils.EncoderTransformerConfig
  max_rows: 5
  max_cols: 5
  num_layers: 6
  transformer_layer:
    _target_:  models.utils.TransformerLayerConfig
    num_heads: 6
    emb_dim_per_head: 12
    mlp_dim_factor: 4.0
    dropout_rate: 0.0
    attention_dropout_rate: 0.0
  latent_dim: 32
  variational: True
  latent_projection_bias: False

decoder_transformer:
  _target_:  models.utils.DecoderTransformerConfig
  max_rows: 5
  max_cols: 5
  num_layers: 2
  transformer_layer:
    _target_:  models.utils.TransformerLayerConfig
    num_heads: 6
    emb_dim_per_head: 12
    mlp_dim_factor: 4.0
    dropout_rate: 0.0
    attention_dropout_rate: 0.0


