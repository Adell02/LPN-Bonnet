wandb:
  entity: ga624-imperial-college-london
  project: LPN-ARC-Structured

encoder_transformer:
  _target_: models.utils.EncoderTransformerConfig
  vocab_size: 10
  max_rows: 30
  max_cols: 30
  transformer_layer:
    _target_: models.utils.TransformerLayerConfig
    dropout_rate: 0.05

decoder_transformer:
  _target_: models.utils.DecoderTransformerConfig
  vocab_size: 10
  max_rows: 30
  max_cols: 30
  transformer_layer:
    _target_: models.utils.TransformerLayerConfig
    dropout_rate: 0.05

structured:
  artifacts:
    models:
      - "ga624-imperial-college-london/LPN-ARC/sparkling-wood-255--checkpoint:latest"
      - "ga624-imperial-college-london/LPN-ARC/revived-grass-259--checkpoint:latest"
      - "ga624-imperial-college-london/LPN-ARC/wobbly-rain-258--checkpoint:latest"  
  alphas: [0.3333333, 0.3333333, 0.3333333]     # uniform over 3 models
  # Optional: configuration describing the model architecture of the loaded checkpoints.
  # If provided, StructuredTrainer will instantiate enc/dec with these hyperparameters
  # to ensure parameter shapes match the artifacts.
  model_config:
    max_rows: 5
    max_cols: 5
    num_layers: 2
    num_heads: 6
    emb_dim_per_head: 12   # total emb dim = num_heads * emb_dim_per_head
    mlp_dim_factor: 4.0
    dropout_rate: 0.0
    attention_dropout_rate: 0.0
    latent_dim: 32
    variational: true
    latent_projection_bias: false

training:
  seed: 0
  mixed_precision: false
  batch_size: 128
  gradient_accumulation_steps: 1
  learning_rate: 1e-3
  linear_warmup_steps: 99
  # Option A: use HF/local datasets
  train_datasets: ""  # leave empty when using struct patterns
  use_hf: true
  online_data_augmentation: false
  inference_mode: "mean"  # mean/all/random_search/gradient_ascent/evolutionary_search
  inference_kwargs: {}
  prior_kl_coeff: null
  pairwise_kl_coeff: null
  total_num_steps: 5000
  log_every_n_steps: 10
  eval_every_n_logs: 1        # null to disable
  save_checkpoint_every_n_logs: 100  # null to disable
  resume_from_checkpoint: null
  # Option B: balanced structured tetromino patterns (O, T, L) - now using task generator
  struct_patterns_balanced: true
  struct_num_pairs: 4          # Increased to match regular training
  num_workers: 4               # Added for task generator
  # Remove struct_length_per_pattern as it's no longer needed with task generator

eval:
  dataset:
    folder: ""  # optional; when empty, can evaluate on a small balanced struct sample
    use_hf: true
  batch_size: 96  # batch size for evaluation (like train.py)
  inference_mode: "mean"
  inference_kwargs: {}
  num_tasks_to_show: 5
  tsne_max_points: 2000


