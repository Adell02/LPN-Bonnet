wandb:
  entity: ga624-imperial-college-london
  project: LPN-ARC-Structured

encoder_transformer:
  _target_: models.utils.EncoderTransformerConfig
  vocab_size: 10
  max_rows: 30
  max_cols: 30
  transformer_layer:
    _target_: models.utils.TransformerLayerConfig
    dropout_rate: 0.05
  variational: true

decoder_transformer:
  _target_: models.utils.DecoderTransformerConfig
  vocab_size: 10
  max_rows: 30
  max_cols: 30
  transformer_layer:
    _target_: models.utils.TransformerLayerConfig
    dropout_rate: 0.05

structured:
  artifacts:
    models:
      - "ga624-imperial-college-london/LPN-ARC/sparkling-wood-255--checkpoint:latest"  # Model Pattern 1
      - "ga624-imperial-college-london/LPN-ARC/revived-grass-259--checkpoint:latest"  # Model Pattern 2
      - "ga624-imperial-college-london/LPN-ARC/wobbly-rain-258--checkpoint:latest"   # Model Pattern 3
  alphas: [0.3333333, 0.3333333, 0.3333333]     # uniform over 3 models
  # Optional: configuration describing the model architecture of the loaded checkpoints.
  # If provided, StructuredTrainer will instantiate enc/dec with these hyperparameters
  # to ensure parameter shapes match the artifacts.
  model_config:
    max_rows: 5
    max_cols: 5
    num_layers: 2
    num_heads: 6
    emb_dim_per_head: 12   # total emb dim = num_heads * emb_dim_per_head
    mlp_dim_factor: 4.0
    dropout_rate: 0.0
    attention_dropout_rate: 0.0
    latent_dim: 32
    variational: true
    latent_projection_bias: false

training:
  seed: 0
  mixed_precision: false
  batch_size: 128
  gradient_accumulation_steps: 1
  learning_rate: 1e-3
  linear_warmup_steps: 99
  # Option A: use HF/local datasets
  train_datasets: ""  # leave empty when using struct patterns
  use_hf: true
  online_data_augmentation: false
  inference_mode: "mean"  # mean/all/random_search/gradient_ascent/evolutionary_search
  inference_kwargs: {}
  prior_kl_coeff: null
  pairwise_kl_coeff: null
  total_num_steps: 5000
  encoder_expose_steps: 5000  # number of initial gradient steps encoders are trainable
  repulsion_kl: 0
  contrastive_kl: 0.01
  log_every_n_steps: 5
  eval_every_n_logs: 20       # null to disable
  save_checkpoint_every_n_logs: null  # null to disable
  resume_from_checkpoint: null
  # Option B: balanced structured tetromino patterns (O, T, L) - now using task generator
  struct_patterns_balanced: true
  struct_num_pairs: 4          # 4 pairs per task (input + output)
  num_workers: 4               # Number of workers for data generation
  pattern_per_task: true
  # Training: 1024 total samples (341 per pattern: O, T, L tetrominos)
  # Evaluation: 96 total samples (32 per pattern: O, T, L tetrominos)

eval:
  dataset:
    folder: ""  # optional; when empty, evaluates on balanced struct sample (32 samples per pattern)
    use_hf: true
  batch_size: 96  # batch size for evaluation (like train.py)
  inference_mode: "mean"
  inference_kwargs: {}
  num_tasks_to_show: 3
  tsne_max_points: 1536  # Should be divisible by 4 (points per group: 3 encoders + 1 context) for clean sampling  
  # Test datasets for comprehensive evaluation across all patterns
  eval_datasets:
    - generator: "STRUCT_PATTERN"
      name: "pattern_1"
      num_pairs: 4
      length: 96  # 32 samples per pattern (O, T, L)
      task_generator_kwargs:
        pattern: 1  # O-tetromino
      batch_size: 96
      num_tasks_to_show: 5
    - generator: "STRUCT_PATTERN"
      name: "pattern_2" 
      num_pairs: 4
      length: 96  # 32 samples per pattern (O, T, L)
      task_generator_kwargs:
        pattern: 2  # T-tetromino
      batch_size: 96
      num_tasks_to_show: 5
    - generator: "STRUCT_PATTERN"
      name: "pattern_3"
      num_pairs: 4
      length: 96  # 32 samples per pattern (O, T, L)
      task_generator_kwargs:
        pattern: 3  # L-tetromino
      batch_size: 96
      num_tasks_to_show: 5
  test_datasets: []


